# credit-risk

# View the structure of loan_data
str(loan_data)

# Load the gmodels package 
install.packages("gmodels")
library(gmodels)

# Call CrossTable() on loan_status
CrossTable(loan_data$loan_status)

# Call CrossTable() on grade and loan_status
CrossTable(loan_data$grade,loan_data$loan_status,prop.r=T,prop.c=F,prop.t=F,prop.chisq=F)
##The proportion of defaults increases when the credit rating moves from A to G.

# Create histogram of loan_amnt: hist_1
hist_1<-hist(loan_data$loan_amnt)

# Print locations of the breaks in hist_1,breaks giving the breakpoints between histogram cells.
hist_1$breaks

# Change number of breaks and add labels: hist_2
hist_2 <- hist(loan_data$loan_amnt, breaks = 200, xlab = "Loan amount", main = "Histogram of the loan amount")


# Plot the age variable
plot(loan_data$age,ylab="Age")

# Save the outlier's index to index_highage
index_highage<-which(loan_data$age>122)

# Create data set new_data with outlier deleted
new_data <- loan_data[-index_highage, ]

# Make bivariate scatterplot of age and annual income
plot(loan_data$age, loan_data$annual_inc, xlab = "Age", ylab = "Annual Income")

# Look at summary of loan_data
summary(loan_data$int_rate)

# Get indices of missing interest rates: na_index
na_index <- which(is.na(loan_data$int_rate))

# Remove observations with missing interest rates: loan_data_delrow_na
loan_data_delrow_na<- loan_data[-na_index, ]

# Make copy of loan_data
loan_data_delcol_na <- loan_data

# Delete interest rate column from loan_data_delcol_na
loan_data_delcol_na$int_rate<-NULL

# Compute the median of int_rate
median_ir<-median(loan_data$int_rate,na.rm=T)

# Make copy of loan_data
loan_data_replace <- loan_data

# Replace missing interest rates with median
loan_data_replace$int_rate[which(is.na(loan_data_replace$int_rate))]<- median_ir

# Check if the NAs are gone
summary(loan_data_replace$int_rate)

# Make the necessary replacements in the coarse classification example below 
loan_data$ir_cat <- rep(NA, length(loan_data$int_rate))

loan_data$ir_cat[which(loan_data$int_rate <= 8)] <- "0-8"
loan_data$ir_cat[which(loan_data$int_rate > 8 & loan_data$int_rate <= 11)] <- "8-11"
loan_data$ir_cat[which(loan_data$int_rate > 11 & loan_data$int_rate <= 13.5)] <- "11-13.5"
loan_data$ir_cat[which(loan_data$int_rate > 13.5)] <- "13.5+"
loan_data$ir_cat[which(is.na(loan_data$int_rate))] <- "Missing"

loan_data$ir_cat <- as.factor(loan_data$ir_cat)

# Look at your new variable using plot()
plot(loan_data$ir_cat)


# Set seed of 567
set.seed(567)

# Store row numbers for training set: index_train
index_train<-sample(1:nrow(loan_data),2/3*nrow(loan_data))

# Create training set: training_set
training_set <- loan_data[index_train, ]

# Create test set: test_set
test_set<-loan_data[-index_train,]


# Create confusion matrix
conf_matrix<-table(test_set$loan_status,model_pred)

# Compute classification accuracy
accuracy<-(conf_matrix[1,1]+conf_matrix[2,2])/(nrow(test_set))
accuracy
# Compute sensitivity
sensitivity<-conf_matrix[2,2]/(conf_matrix[2,1]+conf_matrix[2,2])
sensitivity


# Build a glm model with variable ir_cat as a predictor
log_model_cat<-glm(loan_status~ir_cat,family="binomial",data=training_set)


# Print the parameter estimates 
summary(log_model_cat)

# Look at the different categories in ir_cat using table()
table(loan_data$ir_cat)

# Build the logistic regression model
log_model_multi<-glm(loan_status~age+ir_cat+grade+loan_amnt+annual_inc,family="binomial",data=training_set)


# Obtain significance levels using summary()
summary(log_model_multi)

# Build the logistic regression model
predictions_all_small <- predict(log_model_small, newdata = test_set,type="response")

# Look at the range of the object "predictions_all_small"
range(predictions_all_small)

# Change the code below to construct a logistic regression model using all available predictors in the data set
log_model_small <- glm(loan_status ~., family = "binomial", data = training_set)

# Make PD-predictions for all test set elements using the the full logistic regression model
predictions_all_full<-predict(log_model_small,newdata=test_set,type="response")

# Look at the predictions range
range(predictions_all_full)

# The code for the logistic regression model and the predictions is given below
log_model_full <- glm(loan_status ~ ., family = "binomial", data = training_set)
predictions_all_full <- predict(log_model_full, newdata = test_set, type = "response")

# Make a binary predictions-vector using a cut-off of 15%
pred_cutoff_15<-ifelse(predictions_all_full>0.15,1,0)

# Construct a confusion matrix
table(test_set$loan_status,pred_cutoff_15)

# Fit the logit, probit and cloglog-link logistic regression models
log_model_logit <- glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
                       family = binomial(link = logit), data = training_set)
log_model_probit <- glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
                       family = binomial(link = probit), data = training_set)
log_model_cloglog <-glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
                       family = binomial(link = cloglog), data = training_set)  
  
# Make predictions for all models using the test set
predictions_logit <- predict(log_model_logit, newdata = test_set, type = "response")
predictions_probit <- predict(log_model_probit,newdata=test_set,type="response")
predictions_cloglog <- predict(log_model_cloglog,newdata=test_set,type="response")
  
# Use a cut-off of 14% to make binary predictions-vectors
cutoff <- 0.14
class_pred_logit <- ifelse(predictions_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(predictions_probit>cutoff,1,0)
class_pred_cloglog <- ifelse(predictions_cloglog>cutoff,1,0)
  
# Make a confusion matrix for the three models
tab_class_logit <- table(true_val,class_pred_logit)
tab_class_probit <- table(true_val,class_pred_probit)
tab_class_cloglog <- table(true_val,class_pred_cloglog)
  
# Compute the classification accuracy for all three models
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_set)
acc_probit <- sum(diag(tab_class_probit))/nrow(test_set)
acc_cloglog <- sum(diag(tab_class_cloglog))/nrow(test_set)


# Load package rpart in your workspace.
library("rpart")

# Change the code provided in the video such that a decision tree is constructed using the undersampled training set. Include rpart.control to relax the complexity parameter to 0.001.
tree_undersample <- rpart(loan_status ~ ., method = "class",
                          data =  undersampled_training_set,control=rpart.control(cp=0.001))

# Plot the decision tree
plot(tree_undersample,uniform=TRUE)

# Add labels to the decision tree
text(tree_undersample)

CAs mentioned in the video, you can also change the prior probabilities to obtain a decision tree. This is an indirect way of adjusting the importance of misclassifications for each class. You can specify another argument inside rpart() to include prior probabities. The argument you are looking for has the following form

parms = list(prior=c(non_default_proportion, default_proportion))
The rpart package is now already loaded in your workspace.

Instructions
Change the code provided such that a decision tree is constructed , including the argument parms and changing the proportion of non-defaults to 0.7, and of defaults to 0.3 (they should always sum up to 1). Additionally, include control = rpart.control(cp = 0.001) as well.
Plot the decision tree using the function plot and the tree object name. Add a second argument "uniform=TRUE" to get equal-sized branches.
Add labels to the tree using function text() and the decision tree object name.

# Change the code below such that a tree is constructed with adjusted prior probabilities.
tree_prior <- rpart(loan_status ~ ., method = "class",
                    data = training_set,parms=list(prior=c(0.7,0.3)),control=rpart.control(cp=0.001))

# Plot the decision tree
plot(tree_prior,uniform=TRUE)

# Add labels to the decision tree
text(tree_prior)
               






Including a loss matrix
100xp
Thirdly, you can include a loss matrix, changing the relative importance of misclassifying a default as non-default versus a non-default as a default. You want to stress that misclassifying a default as a non-default should be penalized more heavily. Including a loss matrix can again be done in the argument parms in the loss matrix.

parms = list(loss = matrix(c(0, cost_def_as_nondef, cost_nondef_as_def, 0), ncol=2))
Doing this, you are constructing a 2x2-matrix with zeroes on the diagonal and changed loss penalties off-diagonal. The default loss matrix is all ones off-diagonal.

Instructions
Change the code provided such a loss matrix is included, with a penalization that is 10 times bigger when misclassifying an actual default as a non-default. This can be done replacing cost_def_as_nondef by 10, and cost_nondef_as_def by 1. Similar to what you've done in the previous exercises, include rpart.control to relax the complexity parameter to 0.001.
Plot the decision tree using the function plot and the tree object name. Add a second argument uniform = TRUE to get equal-sized branches, and add labels to the tree using text() with the tree object name.

# Change the code below such that a decision tree is constructed using a loss matrix penalizing 10 times more heavily for misclassified defaults.
tree_loss_matrix <- rpart(loan_status ~ ., method = "class",
                          data =  training_set,parms=list(loss=matrix(c(0,10,1,0),ncol=2)),control=rpart.control(cp=0.001))

# Plot the decision tree
plot(tree_loss_matrix,uniform=TRUE)

# Add labels to the decision tree
text(tree_loss_matrix)


